{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15537ae7-1481-4737-9fca-d6be44dc09ed",
   "metadata": {},
   "source": [
    "### Embedding an Input Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51dccbfb-99c6-4138-8657-ab291307c33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Life is short, eat dessert first'\n",
    "dc = {s:i for i,s \n",
    "      in enumerate(sorted(sentence.replace(',', '').split()))}  #dictonary creation\n",
    "\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cadeac73-fb43-4120-87b9-34894fb6a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# assign an integer index to each word:\n",
    "import torch\n",
    "sentence_int = torch.tensor(\n",
    "    [dc[s] for s in sentence.replace(',', '').split()]\n",
    ")\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900cdd7f-bb9e-42a5-85fe-29440bba00eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [ 0.1794,  1.8951,  0.4954],\n",
      "        [ 0.2692, -0.0770, -1.0205],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-1.1925,  0.6984, -1.4097]])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3 dimensional embedding layer\n",
    "vocab_size = 50_000\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(vocab_size, 3)\n",
    "embedded_sentence = embed(sentence_int).detach() # detach : no gradient computation for this tensor\n",
    "print(embedded_sentence)\n",
    "print(embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fbd81-fc5f-4e24-bc48-219725279e08",
   "metadata": {},
   "source": [
    "### Defining the Weight Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfa091e-c706-4427-913f-f165b98f8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "\n",
    "d_q, d_k, d_v = 2, 2, 4 #here dk and dq have to be equla since we are taking the dot product and hence the no of element in both of them smust match\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d, d_q))\n",
    "W_key = torch.nn.Parameter(torch.rand(d, d_k))\n",
    "W_value = torch.nn.Parameter(torch.rand(d, d_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3803fe3-a150-4657-ac2f-4818a58714f3",
   "metadata": {},
   "source": [
    "### Computing the unnormalised attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0a3e1d-c058-4783-a086-a5ccadda3dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1794, 1.8951, 0.4954])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# for one word embedding x_2\n",
    "x_2 = embedded_sentence[1]\n",
    "print(x_2)\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422b4e2c-b5c0-43a1-b867-9ac4e12b7bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "#Computing keys and values\n",
    "keys = embedded_sentence @ W_key\n",
    "values = embedded_sentence @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e228722-8d53-4c6f-aac9-e17f6086bebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2903, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#compute the unnormalized attention weight for the query and 5th input element (corresponding to index position 4) as follows:\n",
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e714b936-db32-4192-8ee4-082d74509386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "# all unnormalised attention weights for query\n",
    "omega_2 = query_2 @ keys.T\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e7f9ca-bbaf-4bb3-a079-5f6be21e6972",
   "metadata": {},
   "source": [
    "### Computing the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73779d67-0089-4460-9e26-70c19df5995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "667485a1-fd2f-4f45-b60a-f2a42f26b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([0.5313, 1.3607, 0.7891, 1.3110], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "## Computing the context vector:\n",
    "context_vector_2 = attention_weights_2 @ values\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd8c30-01fd-4ff1-a992-c21954e326a8",
   "metadata": {},
   "source": [
    "### Self Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074ed986-2c8b-473d-aea4-33bfdbddf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T  # unnormalized attention weights    \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / self.d_out_kq**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d55700-032d-4c1a-a803-a2932fa0099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1564,  0.1028, -0.0763, -0.0764],\n",
      "        [ 0.5313,  1.3607,  0.7891,  1.3110],\n",
      "        [-0.3542, -0.1234, -0.2626, -0.3706],\n",
      "        [ 0.0071,  0.3345,  0.0969,  0.1998],\n",
      "        [ 0.1008,  0.4780,  0.2021,  0.3674],\n",
      "        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Using the self attention class :\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# reduce d_out_v from 4 to 1, because we have 4 heads\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 4\n",
    "\n",
    "sa = SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "print(sa(embedded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847233a-9176-4e5a-b406-67a9124026c9",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dadf38-51f3-4075-8823-f23eca04f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THe multi head attention class wrapped in slef attention class\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [SelfAttention(d_in, d_out_kq, d_out_v) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1) #concatinate all the heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f512a87-3074-4930-aa71-4372e3e59e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0185],\n",
      "        [ 0.4003],\n",
      "        [-0.1103],\n",
      "        [ 0.0668],\n",
      "        [ 0.1180],\n",
      "        [-0.1827]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A single head\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 1\n",
    "\n",
    "sa = SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "print(sa(embedded_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ff3843-b88e-4713-be1f-17964e4a5cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0185,  0.0170,  0.1999, -0.0860],\n",
      "        [ 0.4003,  1.7137,  1.3981,  1.0497],\n",
      "        [-0.1103, -0.1609,  0.0079, -0.2416],\n",
      "        [ 0.0668,  0.3534,  0.2322,  0.1008],\n",
      "        [ 0.1180,  0.6949,  0.3157,  0.2807],\n",
      "        [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "# USing the multi head attention class\n",
    "torch.manual_seed(123)\n",
    "\n",
    "block_size = embedded_sentence.shape[1]\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out_kq, d_out_v, num_heads=4\n",
    ")\n",
    "\n",
    "context_vecs = mha(embedded_sentence)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e51ab-7707-42cf-a45f-e27e971438a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
